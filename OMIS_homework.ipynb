{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Part A (25%)\n","# 1. Download the “dataset_lm.csv” file from Canvas and upload it to Jupyter Notebook.\n","# 2. Run the OLS model by using the dependent and explanatory variables in the dataset.\n","# 3. Show your summary table in Python and interpret your results in the summary report.\n","# Part B (25%)\n","# 1. Use error values from the OLS model to calculate their standard deviation and\n","# autocorrelation values for the first three lags.\n","# 2. Then, run the GLS model accordingly.\n","# 3. Show your summary table in Python and interpret your results in the summary report.\n","# Part C (25%)\n","# 1. Split the dataset into two as the training and test sets (test size = 0.5).\n","# 2. Run the Lasso model with alpha=1 and estimate the coefficients using the training set.\n","# 3. Then, calculate the mean absolute percentage error using the test set.\n","# 4. Find an approximate value for alpha that minimizes the mean absolute percentage error.\n","# Part D (25%)\n","# 1. Use the demand data given in the table and develop an appropriate forecasting model\n","# (i.e., the tailored regularization discussed in the class—see your slides for more info)\n","# that exploits the available information given in the table as much as possible.\n","# 2. Interpret your results.\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib as mp\n","import statsmodels.api as sm\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import Lasso\n","from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error"]},{"cell_type":"markdown","metadata":{},"source":["# Part A"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# 1. Download the “dataset_lm.csv” file from Canvas and upload it to Jupyter Notebook.\n","\n","df = pd.read_csv('https://raw.githubusercontent.com/EthanRosehart/PM_Assignments/refs/heads/main/dataset_lm.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dependent Var</th>\n","      <th>Explanatory Var #1</th>\n","      <th>Explanatory Var #2</th>\n","      <th>Explanatory Var #3</th>\n","      <th>Explanatory Var #4</th>\n","      <th>Explanatory Var #5</th>\n","      <th>Explanatory Var #6</th>\n","      <th>Explanatory Var #7</th>\n","      <th>Explanatory Var #8</th>\n","      <th>Explanatory Var #9</th>\n","      <th>Explanatory Var #10</th>\n","      <th>Explanatory Var #11</th>\n","      <th>Explanatory Var #12</th>\n","      <th>Explanatory Var #13</th>\n","      <th>Explanatory Var #14</th>\n","      <th>Explanatory Var #15</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>56.293458</td>\n","      <td>13.698667</td>\n","      <td>50.639873</td>\n","      <td>0</td>\n","      <td>-18.568035</td>\n","      <td>45.121911</td>\n","      <td>11.412501</td>\n","      <td>56.410757</td>\n","      <td>2</td>\n","      <td>-12.281132</td>\n","      <td>38.996909</td>\n","      <td>-3.010548</td>\n","      <td>49.195073</td>\n","      <td>0</td>\n","      <td>-21.153143</td>\n","      <td>46.919314</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>58.473431</td>\n","      <td>2.714725</td>\n","      <td>65.845845</td>\n","      <td>1</td>\n","      <td>-25.105932</td>\n","      <td>47.190213</td>\n","      <td>10.080280</td>\n","      <td>65.383107</td>\n","      <td>3</td>\n","      <td>-36.763585</td>\n","      <td>51.654939</td>\n","      <td>4.991111</td>\n","      <td>45.591729</td>\n","      <td>0</td>\n","      <td>-6.474403</td>\n","      <td>53.383508</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>94.195330</td>\n","      <td>11.618072</td>\n","      <td>65.072497</td>\n","      <td>0</td>\n","      <td>-7.897464</td>\n","      <td>52.163036</td>\n","      <td>11.057301</td>\n","      <td>82.812717</td>\n","      <td>0</td>\n","      <td>-15.733547</td>\n","      <td>48.913837</td>\n","      <td>-2.457696</td>\n","      <td>56.608806</td>\n","      <td>0</td>\n","      <td>-27.903299</td>\n","      <td>48.515026</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>29.074583</td>\n","      <td>0.818623</td>\n","      <td>45.408996</td>\n","      <td>1</td>\n","      <td>-18.316132</td>\n","      <td>54.356714</td>\n","      <td>5.029029</td>\n","      <td>48.812471</td>\n","      <td>1</td>\n","      <td>-12.825591</td>\n","      <td>45.851732</td>\n","      <td>14.974177</td>\n","      <td>47.362594</td>\n","      <td>1</td>\n","      <td>-10.064411</td>\n","      <td>55.266254</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>86.035569</td>\n","      <td>9.077544</td>\n","      <td>73.548021</td>\n","      <td>0</td>\n","      <td>-19.204165</td>\n","      <td>47.186807</td>\n","      <td>12.128134</td>\n","      <td>62.520911</td>\n","      <td>2</td>\n","      <td>-13.804860</td>\n","      <td>47.765904</td>\n","      <td>9.593982</td>\n","      <td>53.700562</td>\n","      <td>0</td>\n","      <td>-17.546302</td>\n","      <td>48.150543</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>59.313030</td>\n","      <td>-1.625179</td>\n","      <td>74.795225</td>\n","      <td>0</td>\n","      <td>-27.917682</td>\n","      <td>43.443319</td>\n","      <td>6.971366</td>\n","      <td>78.497117</td>\n","      <td>0</td>\n","      <td>-38.808025</td>\n","      <td>52.181380</td>\n","      <td>16.974969</td>\n","      <td>58.124156</td>\n","      <td>1</td>\n","      <td>-9.023169</td>\n","      <td>55.840461</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>43.302299</td>\n","      <td>17.177045</td>\n","      <td>49.674376</td>\n","      <td>1</td>\n","      <td>-23.490245</td>\n","      <td>58.160871</td>\n","      <td>15.996831</td>\n","      <td>49.845730</td>\n","      <td>3</td>\n","      <td>-24.849489</td>\n","      <td>51.781663</td>\n","      <td>8.667649</td>\n","      <td>72.365524</td>\n","      <td>0</td>\n","      <td>-22.942562</td>\n","      <td>49.120537</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>25.919500</td>\n","      <td>-3.606102</td>\n","      <td>52.838652</td>\n","      <td>0</td>\n","      <td>-20.780413</td>\n","      <td>52.866009</td>\n","      <td>23.282374</td>\n","      <td>68.007436</td>\n","      <td>0</td>\n","      <td>-24.574748</td>\n","      <td>54.423094</td>\n","      <td>8.551908</td>\n","      <td>53.560828</td>\n","      <td>1</td>\n","      <td>-6.769081</td>\n","      <td>48.255181</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>30.353577</td>\n","      <td>4.052598</td>\n","      <td>45.221950</td>\n","      <td>1</td>\n","      <td>-23.692647</td>\n","      <td>44.708397</td>\n","      <td>19.821684</td>\n","      <td>73.394895</td>\n","      <td>3</td>\n","      <td>-28.983295</td>\n","      <td>57.622157</td>\n","      <td>14.581353</td>\n","      <td>55.468061</td>\n","      <td>0</td>\n","      <td>-13.611744</td>\n","      <td>57.230137</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>51.590542</td>\n","      <td>9.369535</td>\n","      <td>55.801956</td>\n","      <td>1</td>\n","      <td>-26.457209</td>\n","      <td>42.325602</td>\n","      <td>-2.058181</td>\n","      <td>74.935838</td>\n","      <td>3</td>\n","      <td>-28.052593</td>\n","      <td>44.794664</td>\n","      <td>2.789562</td>\n","      <td>55.077621</td>\n","      <td>1</td>\n","      <td>-20.948876</td>\n","      <td>55.317270</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Dependent Var  Explanatory Var #1  Explanatory Var #2  Explanatory Var #3  \\\n","0      56.293458           13.698667           50.639873                   0   \n","1      58.473431            2.714725           65.845845                   1   \n","2      94.195330           11.618072           65.072497                   0   \n","3      29.074583            0.818623           45.408996                   1   \n","4      86.035569            9.077544           73.548021                   0   \n","5      59.313030           -1.625179           74.795225                   0   \n","6      43.302299           17.177045           49.674376                   1   \n","7      25.919500           -3.606102           52.838652                   0   \n","8      30.353577            4.052598           45.221950                   1   \n","9      51.590542            9.369535           55.801956                   1   \n","\n","   Explanatory Var #4  Explanatory Var #5  Explanatory Var #6  \\\n","0          -18.568035           45.121911           11.412501   \n","1          -25.105932           47.190213           10.080280   \n","2           -7.897464           52.163036           11.057301   \n","3          -18.316132           54.356714            5.029029   \n","4          -19.204165           47.186807           12.128134   \n","5          -27.917682           43.443319            6.971366   \n","6          -23.490245           58.160871           15.996831   \n","7          -20.780413           52.866009           23.282374   \n","8          -23.692647           44.708397           19.821684   \n","9          -26.457209           42.325602           -2.058181   \n","\n","   Explanatory Var #7  Explanatory Var #8  Explanatory Var #9  \\\n","0           56.410757                   2          -12.281132   \n","1           65.383107                   3          -36.763585   \n","2           82.812717                   0          -15.733547   \n","3           48.812471                   1          -12.825591   \n","4           62.520911                   2          -13.804860   \n","5           78.497117                   0          -38.808025   \n","6           49.845730                   3          -24.849489   \n","7           68.007436                   0          -24.574748   \n","8           73.394895                   3          -28.983295   \n","9           74.935838                   3          -28.052593   \n","\n","   Explanatory Var #10  Explanatory Var #11  Explanatory Var #12  \\\n","0            38.996909            -3.010548            49.195073   \n","1            51.654939             4.991111            45.591729   \n","2            48.913837            -2.457696            56.608806   \n","3            45.851732            14.974177            47.362594   \n","4            47.765904             9.593982            53.700562   \n","5            52.181380            16.974969            58.124156   \n","6            51.781663             8.667649            72.365524   \n","7            54.423094             8.551908            53.560828   \n","8            57.622157            14.581353            55.468061   \n","9            44.794664             2.789562            55.077621   \n","\n","   Explanatory Var #13  Explanatory Var #14  Explanatory Var #15  \n","0                    0           -21.153143            46.919314  \n","1                    0            -6.474403            53.383508  \n","2                    0           -27.903299            48.515026  \n","3                    1           -10.064411            55.266254  \n","4                    0           -17.546302            48.150543  \n","5                    1            -9.023169            55.840461  \n","6                    0           -22.942562            49.120537  \n","7                    1            -6.769081            48.255181  \n","8                    0           -13.611744            57.230137  \n","9                    1           -20.948876            55.317270  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Verify Dataset \n","\n","df.head(10)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 422 entries, 0 to 421\n","Data columns (total 16 columns):\n"," #   Column               Non-Null Count  Dtype  \n","---  ------               --------------  -----  \n"," 0   Dependent Var        422 non-null    float64\n"," 1   Explanatory Var #1   422 non-null    float64\n"," 2   Explanatory Var #2   422 non-null    float64\n"," 3   Explanatory Var #3   422 non-null    int64  \n"," 4   Explanatory Var #4   422 non-null    float64\n"," 5   Explanatory Var #5   422 non-null    float64\n"," 6   Explanatory Var #6   422 non-null    float64\n"," 7   Explanatory Var #7   422 non-null    float64\n"," 8   Explanatory Var #8   422 non-null    int64  \n"," 9   Explanatory Var #9   422 non-null    float64\n"," 10  Explanatory Var #10  422 non-null    float64\n"," 11  Explanatory Var #11  422 non-null    float64\n"," 12  Explanatory Var #12  422 non-null    float64\n"," 13  Explanatory Var #13  422 non-null    int64  \n"," 14  Explanatory Var #14  422 non-null    float64\n"," 15  Explanatory Var #15  422 non-null    float64\n","dtypes: float64(13), int64(3)\n","memory usage: 52.9 KB\n"]}],"source":["df.info()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                                 OLS Regression Results                                \n","=======================================================================================\n","Dep. Variable:          Dependent Var   R-squared (uncentered):                   0.999\n","Model:                            OLS   Adj. R-squared (uncentered):              0.999\n","Method:                 Least Squares   F-statistic:                          4.509e+04\n","Date:                Thu, 17 Oct 2024   Prob (F-statistic):                        0.00\n","Time:                        20:43:40   Log-Likelihood:                         -841.76\n","No. Observations:                 422   AIC:                                      1714.\n","Df Residuals:                     407   BIC:                                      1774.\n","Df Model:                          15                                                  \n","Covariance Type:            nonrobust                                                  \n","=======================================================================================\n","                          coef    std err          t      P>|t|      [0.025      0.975]\n","---------------------------------------------------------------------------------------\n","Explanatory Var #1      1.3078      0.013    102.159      0.000       1.283       1.333\n","Explanatory Var #2      1.7667      0.009    203.101      0.000       1.750       1.784\n","Explanatory Var #3      6.6083      0.177     37.232      0.000       6.259       6.957\n","Explanatory Var #4      2.0725      0.011    189.896      0.000       2.051       2.094\n","Explanatory Var #5     -0.7776      0.011    -68.372      0.000      -0.800      -0.755\n","Explanatory Var #6      0.0124      0.008      1.595      0.111      -0.003       0.028\n","Explanatory Var #7      0.0286      0.006      4.890      0.000       0.017       0.040\n","Explanatory Var #8      0.3510      0.080      4.398      0.000       0.194       0.508\n","Explanatory Var #9     -0.0293      0.010     -2.838      0.005      -0.050      -0.009\n","Explanatory Var #10     0.1292      0.011     11.409      0.000       0.107       0.152\n","Explanatory Var #11     0.0145      0.013      1.152      0.250      -0.010       0.039\n","Explanatory Var #12     0.0752      0.009      8.698      0.000       0.058       0.092\n","Explanatory Var #13     0.7414      0.177      4.198      0.000       0.394       1.089\n","Explanatory Var #14    -0.0218      0.011     -1.974      0.049      -0.043   -8.61e-05\n","Explanatory Var #15     0.1212      0.012     10.324      0.000       0.098       0.144\n","==============================================================================\n","Omnibus:                        0.087   Durbin-Watson:                   1.875\n","Prob(Omnibus):                  0.957   Jarque-Bera (JB):                0.188\n","Skew:                          -0.004   Prob(JB):                        0.910\n","Kurtosis:                       2.897   Cond. No.                         287.\n","==============================================================================\n","\n","Notes:\n","[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n","[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"]}],"source":["# 2. Run the OLS model by using the dependent and explanatory variables in the dataset.\n","\n","# Defining the dependent and independent variables\n","X_columns = [\n","    'Explanatory Var #1', 'Explanatory Var #2', 'Explanatory Var #3', 'Explanatory Var #4',\n","    'Explanatory Var #5', 'Explanatory Var #6', 'Explanatory Var #7', 'Explanatory Var #8',\n","    'Explanatory Var #9', 'Explanatory Var #10', 'Explanatory Var #11', 'Explanatory Var #12',\n","    'Explanatory Var #13', 'Explanatory Var #14', 'Explanatory Var #15'\n","]\n","\n","X = df[X_columns]\n","y = df['Dependent Var']\n","\n","# Fit the OLS model\n","model = sm.OLS(y, X).fit()\n","\n","# Output the summary of the model\n","print(model.summary())"]},{"cell_type":"markdown","metadata":{},"source":["# 3. interpret your results in the summary report.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Part B"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Standard Deviation of Residuals: 1.777835908488389\n","Autocorrelation (Lag 1): 0.05109731118154993\n","Autocorrelation (Lag 2): 0.0555385333342641\n","Autocorrelation (Lag 3): 0.032572809540644745\n"]}],"source":["# 1. Use error values from the OLS model to calculate their standard deviation and\n","# autocorrelation values for the first three lags.\n","\n","# Calculate error values (residuals) from the OLS model\n","residuals = model.resid\n","\n","# Standard deviation of residuals\n","std_dev_residuals = residuals.std()\n","\n","# Autocorrelation for the first three lags\n","autocorrelation_lags = [residuals.autocorr(lag=i) for i in range(1, 4)]\n","\n","# Print results\n","print(f\"Standard Deviation of Residuals: {std_dev_residuals}\")\n","print(f\"Autocorrelation (Lag 1): {autocorrelation_lags[0]}\")\n","print(f\"Autocorrelation (Lag 2): {autocorrelation_lags[1]}\")\n","print(f\"Autocorrelation (Lag 3): {autocorrelation_lags[2]}\")"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                                 GLS Regression Results                                \n","=======================================================================================\n","Dep. Variable:          Dependent Var   R-squared (uncentered):                   0.999\n","Model:                            GLS   Adj. R-squared (uncentered):              0.999\n","Method:                 Least Squares   F-statistic:                          4.509e+04\n","Date:                Thu, 17 Oct 2024   Prob (F-statistic):                        0.00\n","Time:                        20:44:01   Log-Likelihood:                         -841.76\n","No. Observations:                 422   AIC:                                      1714.\n","Df Residuals:                     407   BIC:                                      1774.\n","Df Model:                          15                                                  \n","Covariance Type:            nonrobust                                                  \n","=======================================================================================\n","                          coef    std err          t      P>|t|      [0.025      0.975]\n","---------------------------------------------------------------------------------------\n","Explanatory Var #1      1.3078      0.013    102.159      0.000       1.283       1.333\n","Explanatory Var #2      1.7667      0.009    203.101      0.000       1.750       1.784\n","Explanatory Var #3      6.6083      0.177     37.232      0.000       6.259       6.957\n","Explanatory Var #4      2.0725      0.011    189.896      0.000       2.051       2.094\n","Explanatory Var #5     -0.7776      0.011    -68.372      0.000      -0.800      -0.755\n","Explanatory Var #6      0.0124      0.008      1.595      0.111      -0.003       0.028\n","Explanatory Var #7      0.0286      0.006      4.890      0.000       0.017       0.040\n","Explanatory Var #8      0.3510      0.080      4.398      0.000       0.194       0.508\n","Explanatory Var #9     -0.0293      0.010     -2.838      0.005      -0.050      -0.009\n","Explanatory Var #10     0.1292      0.011     11.409      0.000       0.107       0.152\n","Explanatory Var #11     0.0145      0.013      1.152      0.250      -0.010       0.039\n","Explanatory Var #12     0.0752      0.009      8.698      0.000       0.058       0.092\n","Explanatory Var #13     0.7414      0.177      4.198      0.000       0.394       1.089\n","Explanatory Var #14    -0.0218      0.011     -1.974      0.049      -0.043   -8.61e-05\n","Explanatory Var #15     0.1212      0.012     10.324      0.000       0.098       0.144\n","==============================================================================\n","Omnibus:                        0.087   Durbin-Watson:                   1.875\n","Prob(Omnibus):                  0.957   Jarque-Bera (JB):                0.188\n","Skew:                          -0.004   Prob(JB):                        0.910\n","Kurtosis:                       2.897   Cond. No.                         287.\n","==============================================================================\n","\n","Notes:\n","[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n","[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"]}],"source":["# 2. Then, run the GLS model accordingly.\n","\n","# Fit the GLS model without adding a constant\n","model_gls = sm.GLS(y, X).fit()\n","\n","# Output the summary of the GLS model\n","print(model_gls.summary())"]},{"cell_type":"markdown","metadata":{},"source":["# 3. interpret your results in the summary report."]},{"cell_type":"markdown","metadata":{},"source":["# Part C"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X_train shape: (211, 15)\n","X_test shape: (211, 15)\n","y_train shape: (211,)\n","y_test shape: (211,)\n"]}],"source":["# 1. Split the dataset into two as the training and test sets (test size = 0.5).\n","\n","# Split the dataset into training and test sets (50% test size)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n","\n","# Display the sizes of the training and test sets\n","print(f\"X_train shape: {X_train.shape}\")\n","print(f\"X_test shape: {X_test.shape}\")\n","print(f\"y_train shape: {y_train.shape}\")\n","print(f\"y_test shape: {y_test.shape}\")"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 1.26972628  1.68394638  2.02626245  2.08756512 -0.91746375 -0.\n","  0.         -0.          0.         -0.          0.01314162  0.\n"," -0.          0.         -0.03617731]\n"]}],"source":["# 2. Run the Lasso model with alpha=1 and estimate the coefficients using the training set.\n","\n","# Initialize the Lasso model with alpha = 1\n","lasso_model = Lasso(alpha=1)\n","\n","# Fit the model using the training data\n","lasso_model.fit(X_train, y_train)\n","\n","# Retrieve and display the estimated coefficients\n","print(lasso_model.coef_)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Absolute Percentage Error (MAPE): 0.04432190198291579\n"]}],"source":["# 3. Then, calculate the mean absolute percentage error using the test set.\n","\n","# Predict using the test set\n","y_pred = lasso_model.predict(X_test)\n","\n","# Calculate the Mean Absolute Percentage Error (MAPE)\n","mape = mean_absolute_percentage_error(y_test, y_pred)\n","\n","print(f\"Mean Absolute Percentage Error (MAPE): {mape}\")"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best alpha: 0.001\n","Best MAPE: 3.940341713313293e-05\n"]}],"source":["# 4. Find an approximate value for alpha that minimizes the mean absolute percentage error.\n","\n","# Set up a range of alpha values to test\n","alpha_range = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n","\n","# Set up Lasso with GridSearch to find the best alpha\n","lasso_grid = GridSearchCV(Lasso(), alpha_range, scoring='neg_mean_absolute_percentage_error', cv=5)\n","\n","# Fit the model on the training data\n","lasso_grid.fit(X_train, y_train)\n","\n","# Get the best alpha value\n","best_alpha = lasso_grid.best_params_['alpha']\n","\n","# Get the corresponding MAPE for the best alpha\n","best_mape = -lasso_grid.best_score_\n","\n","print(f\"Best alpha: {best_alpha}\")\n","print(f\"Best MAPE: {best_mape}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Part D"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Squared Error: 2280.6\n","    Month  Actual Demand  Predicted Demand  Final Forecast (with condition)\n","20     21            144        117.150036                               91\n","21     22            202        188.019571                              202\n","22     23            158        126.088536                              105\n","23     24            160        123.534679                              101\n","24     25            144        120.342357                               96\n"]}],"source":["# 1. Use the demand data given in the table and develop an appropriate forecasting model\n","\n","# I have no idea if this is right, we were way too drunk when we did this, but ya if advance demand\n","# is more than 40% then we use advance demand, cause he said that in class, will review this tomorrow when sober.\n","\n","# Input the data from the image manually\n","data = {\n","    'Month': list(range(1, 26)),\n","    'Demand': [100, 112, 107, 103, 91, 85, 84, 85, 79, 81,\n","               134, 86, 99, 89, 111, 114, 118, 163, 193, 143,\n","               144, 202, 158, 160, 144],\n","    'Advance_Demand': [71, 30, 75, 64, 41, 51, 42, 51, 57, 49,\n","                       134, 52, 99, 56, 81, 79, 73, 163, 193, 99,\n","                       91, 202, 105, 101, 96]\n","}\n","\n","# Create a DataFrame\n","df = pd.DataFrame(data)\n","\n","# Define features and target variable\n","X = df[['Advance_Demand']]\n","y = df['Demand']\n","\n","# Split the data into training and test sets (20 months for training, 5 for testing)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n","\n","# Initialize Lasso model with regularization parameter alpha (tunable)\n","lasso = Lasso(alpha=0.1)\n","\n","# Fit the model on the training data\n","lasso.fit(X_train, y_train)\n","\n","# Make predictions on the test data\n","y_pred = lasso.predict(X_test)\n","\n","# Apply the condition where if advance demand is more than 40% of forecasted demand,\n","# use the advance demand instead of the forecasted value.\n","final_predictions = []\n","for i in range(len(y_pred)):\n","    forecasted = y_pred[i]\n","    advance = X_test.iloc[i]['Advance_Demand']\n","    if advance > 0.4 * forecasted:\n","        final_predictions.append(advance)\n","    else:\n","        final_predictions.append(forecasted)\n","\n","# Calculate mean squared error as a metric\n","mse = mean_squared_error(y_test, final_predictions)\n","print(f'Mean Squared Error: {mse}')\n","\n","# Create a DataFrame for comparison of actual and predicted values\n","forecast = pd.DataFrame({\n","    'Month': df['Month'].iloc[X_test.index],\n","    'Actual Demand': y_test,\n","    'Predicted Demand': y_pred,\n","    'Final Forecast (with condition)': final_predictions\n","})\n","\n","print(forecast)"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Interpret your results in the summary report"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
